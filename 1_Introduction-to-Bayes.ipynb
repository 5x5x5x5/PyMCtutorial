{
 "metadata": {
  "name": "",
  "signature": "sha256:d51d38daeda57a5686ba771b3271e115eea0f4d7460386829b5e07c42f8b2ce3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "An Introduction to Bayesian Statistical Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we jump in to model-building and using MCMC to do wonderful things, it is useful to understand a few of the theoretical underpinnings of the Bayesian statistical paradigm. A little theory (and I do mean a *little*) goes a long way towards being able to apply the methods correctly and effectively.\n",
      "\n",
      "There are several introductory references to Bayesian statistics that go well beyond what we will cover here.\n",
      "\n",
      "![books](http://d.pr/i/v0ea+)\n",
      "\n",
      "## What *is* Bayesian Statistical Analysis?\n",
      "\n",
      "Though many of you will have taken a statistics course or two during your undergraduate (or graduate) education, most of those who have will likely not have had a course in *Bayesian* statistics. Most introductory courses, particularly for non-statisticians, still do not cover Bayesian methods at all, except perhaps to derive Bayes' formula as a trivial rearrangement of the definition of conditional probability. Even today, Bayesian courses are typically tacked onto the curriculum, rather than being integrated into the program.\n",
      "\n",
      "In fact, Bayesian statistics is not just a particular method, or even a class of methods; it is an entirely different paradigm for doing statistical analysis.\n",
      "\n",
      "> Practical methods for making inferences from data using probability models for quantities we observe and about which we wish to learn.\n",
      "*-- Gelman et al. 2013*\n",
      "\n",
      "A Bayesian model is described by parameters, uncertainty in those parameters is described using probability distributions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All conclusions from Bayesian statistical procedures are stated in terms of *probability statements*\n",
      "\n",
      "![](images/prob_model.png)\n",
      "\n",
      "Key characteristic of Bayes\n",
      "\n",
      "- ease of interpretation, summarization of uncertainty\n",
      "- can incorporate uncertainty in parent parameters\n",
      "- easy to calculate summary statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bayesian vs Frequentist Statistics: What's the difference?\n",
      "\n",
      "See the [VanderPlas talk on Tuesday](https://conference.scipy.org/scipy2014/schedule/presentation/444/).\n",
      "\n",
      "![can of worms](images/can-of-worms.jpg)\n",
      "\n",
      "Any statistical paradigm, Bayesian or otherwise, involves at least the following: \n",
      "\n",
      "1. Some **unknown quantities** about which we are interested in learning or testing. We call these *parameters*.\n",
      "2. Some **data** which have been observed, and hopefully contain information about (1).\n",
      "3. One or more **models** that relate the data to the parameters, and is the instrument that is used to learn.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The Frequentist World View\n",
      "\n",
      "![Fisher](images/fisher.png)\n",
      "\n",
      "- The data that have been observed are considered **random**, because they are realizations of random outcomes, and hence will vary each time one goes to observe the system.\n",
      "- Model parameters are considered **fixed**. The parameters' values are unknown, but they are fixed, and so we *condition* on them.\n",
      "\n",
      "In mathematical notation, this implies a (very) general model of the following form:\n",
      "\n",
      "<div style=\"font-size:35px\">\n",
      "\\\\[f(y | \\theta)\\\\]\n",
      "</div>\n",
      "\n",
      "Here, the model \\\\(f\\\\) accepts data values \\\\(y\\\\) as an argument, conditional on particular values of \\\\(\\theta\\\\).\n",
      "\n",
      "Frequentist inference typically involves deriving **estimators** for the unknown parameters. Estimators are formulae that return estimates for particular estimands, as a function of data. They are chosen based on some chosen optimality criterion, such as *unbiasedness*, *variance minimization*, or *efficiency*.\n",
      "\n",
      "For example, lets say that we have collected some data on the prevalence of autism spectrum disorder (ASD) in some defined population. Our sample includes \\\\(n\\\\) sampled children, \\\\(y\\\\) of them having been diagnosed with autism. A frequentist estimator of the prevalence \\\\(p\\\\) is:\n",
      "\n",
      "\\\\[\\hat{p} = \\frac{y}{n}\\\\]\n",
      "\n",
      "Why this particular function? Because it can be shown to be unbiased and minimum-variance.\n",
      "\n",
      "It is important to note that new estimators need to be derived for every estimand that is introduced.\n",
      "\n",
      "### The Bayesian World View\n",
      "\n",
      "![Bayes](images/bayes.png)\n",
      "\n",
      "- Data are considered **fixed**. They used to be random, but once they were written into your lab notebook/spreadsheet/IPython notebook they do not change.\n",
      "- Model parameters themselves may not be random, but Bayesians use probability distribtutions to describe their uncertainty in parameter values, and are therefore treated as **random**. In some cases, it is useful to consider parameters as having been sampled from probability distributions.\n",
      "\n",
      "This implies the following form:\n",
      "\n",
      "<div style=\"font-size:35px\">\n",
      "\\\\[p(\\theta | y)\\\\]\n",
      "</div>\n",
      "\n",
      "This formulation used to be referred to as ***inverse probability***, because it infers from observations to parameters, or from effects to causes.\n",
      "\n",
      "Bayesians do not seek new estimators for every estimation problem they encounter. There is only one estimator for Bayesian inference: **Bayes' Formula**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bayesian Inference, in 3 Easy Steps\n",
      "\n",
      "![123](images/123.png)\n",
      "\n",
      "Gelman et al. (2013) describe the process of conducting Bayesian statistical analysis in 3 steps.\n",
      "\n",
      "### Step 1: Specify a probability model\n",
      "\n",
      "As was noted above, Bayesian statistics involves using probability models to solve problems. So, the first task is to *completely specify* the model in terms of probability distributions. This includes everything: unknown parameters, data, covariates, missing data, predictions. All must be assigned some probability density.\n",
      "\n",
      "This step involves making choices.\n",
      "\n",
      "- what is the form of the sampling distribution of the data?\n",
      "- what form best describes our uncertainty in the unknown parameters?\n",
      "\n",
      "### Step 2: Calculate a posterior distribution\n",
      "\n",
      "The mathematical form \\\\(p(\\theta | y)\\\\) that we associated with the Bayesian approach is referred to as a **posterior distribution**.\n",
      "\n",
      "> posterior /pos\u00b7ter\u00b7i\u00b7or/ (pos-t\u0113r\u00b4e-er) later in time; subsequent.\n",
      "\n",
      "Why posterior? Because it tells us what we know about the unknown \\\\(\\theta\\\\) *after* having observed \\\\(y\\\\).\n",
      "\n",
      "This posterior distribution is formulated as a function of the probability model that was specified in Step 1. Usually, we can write it down but we cannot calculate it analytically. In fact, the difficulty inherent in caclulating the posterior distribution for most models of interest is perhaps the major contributing factor for the lack of widespread adoption of Bayesian methods for data analysis. Various strategies for doing so comprise this tutorial."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB. Bayesian Data Analysis, Third Edition. CRC Press; 2013."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width: 90%;\n",
        "/*        margin-left:auto;*/\n",
        "/*        margin-right:auto;*/\n",
        "    }\n",
        "    ul {\n",
        "        line-height: 145%;\n",
        "        font-size: 90%;\n",
        "    }\n",
        "    li {\n",
        "        margin-bottom: 1em;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: Helvetica, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top: 12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 130%;\n",
        "        width: 90%;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "/*    .prompt{\n",
        "        display: None;\n",
        "    }*/\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: 0.5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "\n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }\n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML object>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}